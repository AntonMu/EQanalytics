{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOAL\n",
    "The goal of this notebook is to first clean up two location databases, one of all addresses in Oakland, and one of vulnerable buildings. We also use similarity search two match similar street names and cluster similar statuses of buidlings.\n",
    "Secondly, we download the image data to individual folders to enable learning later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install -q pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up mixed types of house numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muehlemann/Dropbox/eq/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_openaddr = pd.read_csv('Locations/alameda.csv')\n",
    "def ignore_non_int(df, column = 'NUMBER'):\n",
    "    \"\"\"Returns a df with all rows removed that \n",
    "    do not have integers in 'column'\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.Dataframe \n",
    "        The input dataframe that contains\n",
    "        'column'\n",
    "    column : str, optional\n",
    "        The name of the column that should\n",
    "        only contain integers\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dataframe with rows removed\n",
    "    \"\"\"\n",
    "    def to_int(x):\n",
    "        try:\n",
    "            return int(x)\n",
    "        except:\n",
    "            return x\n",
    "    df[column]=df[column].apply(lambda x: to_int(x))\n",
    "    df['dtypes']=df[column].apply(lambda x: type(x))\n",
    "    df=df.loc[df['dtypes']==type(1)]\n",
    "    df.drop('dtypes',axis=1,inplace=True)\n",
    "    df[column]=df[column].apply(lambda x: to_int(x))\n",
    "    return df\n",
    "df_openaddr = ignore_non_int(df_openaddr)\n",
    "df_openaddr['NUMBER'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Oakland list of vulnerable buildings and do some pre_processings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vul = pd.read_csv('Locations/Oakland.csv')\n",
    "\n",
    "#Seperate Street and number\n",
    "df_vul[['NUMBER','STREET']] = pd.DataFrame(df_vul['address'].str.split(' ',1).tolist(),\n",
    "                                   columns = ['NUMBER','STREET'])\n",
    "#Seperate Street and building\n",
    "df_vul[['STREET','BUILDING']]=pd.DataFrame(df_vul['STREET'].str.split(' - ',1).tolist(),\n",
    "                                   columns = ['STREET','BUILDING'])\n",
    "# Find all unique street names\n",
    "unique_streets = df_vul['STREET'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match street names in df_vul with those in df_openaddr via similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Find all unique streets from openaddr in Oakland\n",
    "all_streets = df_openaddr[df_openaddr['CITY']=='OAKLAND']['STREET'].unique()\n",
    "\n",
    "# Find most similar street names in openaddr db\n",
    "import difflib\n",
    "matching_streets = []\n",
    "for unique_street in unique_streets:\n",
    "    matching_streets.append(difflib.get_close_matches(unique_street, all_streets)[0])\n",
    "\n",
    "#Create a dictionary that can translate street names\n",
    "vul_to_all = dict(zip(unique_streets,matching_streets))\n",
    "\n",
    "# Rename all street names in df_vul to the notation of openaddr\n",
    "df_vul['STREET']=df_vul['STREET'].apply(lambda x: vul_to_all[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find housenumbers of buildings which were combined, i.e. Telegraph Ave 5678+5683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the odd numbers into seperate columns\n",
    "def tidy_split(df, column, sep='|', keep=False):\n",
    "    \"\"\"\n",
    "    Split the values of a column and expand so the new DataFrame has one split\n",
    "    value per row. Filters rows where the column is missing.\n",
    "    \n",
    "    https://github.com/cognoma/genes/blob/721204091a96e55de6dcad165d6d8265e67e2a48/2.process.py\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with the column to split and expand\n",
    "    column : str\n",
    "        the column to split and expand\n",
    "    sep : str\n",
    "        the string used to split the column's values\n",
    "    keep : bool\n",
    "        whether to retain the presplit value as it's own row\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Returns a dataframe with the same columns as `df`.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    new_values = list()\n",
    "    df = df.dropna(subset=[column])\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        values = presplit.split(sep)\n",
    "        if keep and len(values) > 1:\n",
    "            indexes.append(i)\n",
    "            new_values.append(presplit)\n",
    "        for value in values:\n",
    "            indexes.append(i)\n",
    "            new_values.append(value)\n",
    "    new_df = df.iloc[indexes, :].copy()\n",
    "    new_df[column] = new_values\n",
    "    return new_df\n",
    "\n",
    "\n",
    "df_vul=tidy_split(df_vul,'NUMBER',sep='+')\n",
    "df_vul=ignore_non_int(df_vul)\n",
    "df_vul['NUMBER'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure most addresses from df_vul are contained in df_openaddr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "df_openaddr_oakland = df_openaddr[df_openaddr['CITY']=='OAKLAND']\n",
    "missing = []\n",
    "for index,row in df_vul.iterrows():\n",
    "    if not (((df_openaddr_oakland['STREET'] == row['STREET'])\n",
    "            & (df_openaddr_oakland['NUMBER'] == row['NUMBER'])).any()):\n",
    "            missing.append(index)\n",
    "print(len(missing))\n",
    "# Do some manual inspection of the problem childs\n",
    "# df_vul.iloc[missing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just 33 are missing - that is ok for now. Next we find all streets that have at least 5 vulnerable buildings and take find all other buildings in those streets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=pd.DataFrame(df_vul['STREET'].value_counts(dropna=False))\n",
    "vul_streets = counts[counts>=5].dropna().index.tolist()\n",
    "df_openaddr_oakland_vul_streets = df_openaddr_oakland[df_openaddr_oakland['STREET'].isin(vul_streets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that there are no vulnerable buildings in the vulnerable streets\n",
    "\n",
    "def in_which_df(df1,df2,columns = ['STREET','NUMBER'], output = 'first'):\n",
    "    \"\"\"Returns a df with rows only contained in df1 (first), \n",
    "    in both (both), or only in df2 (right) by comparing 'columns'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df1 : pd.Dataframe \n",
    "        The first input dataframe \n",
    "    df2 : pd.Dataframe \n",
    "        The second input dataframe \n",
    "    columns : list of str, optional\n",
    "        The name of the columns where\n",
    "        df1 and df2 should coincide.\n",
    "    output: str, optional\n",
    "        'first': returns rows that are only in df1\n",
    "        'both': returns rows that are both in df1 and df2\n",
    "        'second': returns rows that are only in df2\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame of rows that are only in 'output'\n",
    "        \n",
    "    \"\"\"    \n",
    "    df=df1.drop_duplicates().merge(df2.drop_duplicates(),on=columns,\n",
    "                                      how='outer', indicator=True)\n",
    "    if output == 'first':\n",
    "        return df[df['_merge']=='left_only']\n",
    "    elif output == 'second':\n",
    "        return df[df['_merge']=='right_only']\n",
    "    elif output == 'both':\n",
    "        return df[df['_merge']=='both']\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a list of non-vulnerable buildings from streets that contain more than 5 vulnerable buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LON</th>\n",
       "      <th>LAT</th>\n",
       "      <th>NUMBER</th>\n",
       "      <th>STREET</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>CITY</th>\n",
       "      <th>DISTRICT</th>\n",
       "      <th>REGION</th>\n",
       "      <th>POSTCODE</th>\n",
       "      <th>ID</th>\n",
       "      <th>HASH</th>\n",
       "      <th>Numberlen</th>\n",
       "      <th>parcel_number</th>\n",
       "      <th>address</th>\n",
       "      <th>status_long</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>status_short</th>\n",
       "      <th>BUILDING</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.261672</td>\n",
       "      <td>37.837729</td>\n",
       "      <td>5132</td>\n",
       "      <td>TELEGRAPH AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OAKLAND</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94609</td>\n",
       "      <td>14-1226-15</td>\n",
       "      <td>2b13400a4d483c21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.261910</td>\n",
       "      <td>37.837433</td>\n",
       "      <td>5110</td>\n",
       "      <td>TELEGRAPH AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OAKLAND</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94609</td>\n",
       "      <td>14-1226-15</td>\n",
       "      <td>fe2c0215b85bd442</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LON        LAT  NUMBER        STREET UNIT     CITY  DISTRICT  \\\n",
       "0 -122.261672  37.837729    5132  TELEGRAPH AV  NaN  OAKLAND       NaN   \n",
       "1 -122.261910  37.837433    5110  TELEGRAPH AV  NaN  OAKLAND       NaN   \n",
       "\n",
       "   REGION POSTCODE          ID              HASH  Numberlen parcel_number  \\\n",
       "0     NaN    94609  14-1226-15  2b13400a4d483c21        4.0           NaN   \n",
       "1     NaN    94609  14-1226-15  fe2c0215b85bd442        4.0           NaN   \n",
       "\n",
       "  address status_long  latitude  longitude status_short BUILDING     _merge  \n",
       "0     NaN         NaN       NaN        NaN          NaN      NaN  left_only  \n",
       "1     NaN         NaN       NaN        NaN          NaN      NaN  left_only  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_vul_df = in_which_df(df_openaddr_oakland,df_vul)\n",
    "non_vul_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Street View Images for Datasets via Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "def download_images(df,api_key='', dir_path ='images',class_name = 'vulnerable', \n",
    "                    title = 'view', category = '',pitch=10):\n",
    "    \"\"\"Downloads all Street view for all rows with addresses in df. \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.Dataframe \n",
    "        The df with rows that contain at least the columns\n",
    "        'CITY', 'STREET', 'NUMBER', 'LAT', 'LON'\n",
    "    api_key : str, optional\n",
    "        Google Maps Street View API key, without the key only \n",
    "        the request url is returned\n",
    "    dir_path : str, optional\n",
    "        The path of the download folder\n",
    "    class_name : str, optional\n",
    "        The label of the data to download\n",
    "    title : str, optional\n",
    "        The name of the image file\n",
    "    category : str, optional\n",
    "        An additional layer of nesting within\n",
    "        the class\n",
    "    pitch : int, optional\n",
    "        the camera angle of the street view image,\n",
    "        0 is horizontal\n",
    "    output: str, optional\n",
    "        'first': returns rows that are only in df1\n",
    "        'both': returns rows that are both in df1 and df2\n",
    "        'second': returns rows that are only in df2\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Downloads all images to their respective folders as jpg \n",
    "    and saves all other info in a json file with the same name.\n",
    "    returns True once completed. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def empty_str(x):\n",
    "        if str(x)=='nan':\n",
    "            return ''\n",
    "        else:\n",
    "            return x\n",
    "    total_path = ('/').join([dir_path,class_name])\n",
    "    if not path.isdir(total_path):\n",
    "        makedirs(total_path)\n",
    "    if category:\n",
    "            category = category.replace(' ','-').replace('/','-')\n",
    "            total_path=('/').join([total_path,category])\n",
    "            if not path.isdir(total_path):\n",
    "                makedirs(total_path)\n",
    "        \n",
    "        \n",
    "    for index,row in df.iterrows():\n",
    "        address_str = ('+').join([str(row['NUMBER']),\n",
    "                                        row['STREET'],\n",
    "                                        row['CITY'],\n",
    "                                            ]).replace(' ','+')\n",
    "        try:\n",
    "            unit = empty_str(row['UNIT'])\n",
    "            address_str+='+'+unit\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            postcode = str(row['POSTCODE'])\n",
    "            address_str+='+'+postcode\n",
    "        except:\n",
    "            pass                                 \n",
    "        file_name = ('_').join([title,\n",
    "                                class_name,\n",
    "                                str(int(time.time())),\n",
    "                                \"{:+f}\".format(row['LAT']),\n",
    "                                \"{:+f}\".format(row['LON']),\n",
    "                                str(pitch),\n",
    "                                address_str\n",
    "                               ]) \n",
    "        if category:\n",
    "            file_name+='+'+category\n",
    "\n",
    "        url='https://maps.googleapis.com/maps/api/streetview?source=outdoor&size=640x640'\n",
    "        url+= '&pitch='+str(pitch)\n",
    "        url+= '&key='+api_key\n",
    "        url+= '&location='+address_str\n",
    "        if not api_key:\n",
    "            return url\n",
    "        response = requests.get(url)\n",
    "        file_path = path.join(total_path,file_name)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path+\".jpg\", 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            with open(file_path+\".json\", 'w') as f:\n",
    "                f.write(str(row.to_json()))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Non-vulnerable buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_images(non_vul_df,class_name='non_vulnerable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download vulnerable buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First rename some columns and set city name\n",
    "df_vul=df_vul.rename(index=str, columns={\"latitude\": \"LAT\", \"longitude\": \"LON\"})\n",
    "df_vul['CITY']='OAKLAND'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine different categories of building descriptions via similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = list(pd.DataFrame(df_vul['status_long'].value_counts()).index)\n",
    "\n",
    "# Find similar clusters\n",
    "logical_sets = []\n",
    "for cat in cats:\n",
    "    cluster = difflib.get_close_matches(cat, cats, n=10, cutoff=0.8)\n",
    "    cluster.sort()\n",
    "    logical_sets.append(cluster)\n",
    "\n",
    "    # Delete duplicates\n",
    "unique_list = []\n",
    "for logical_set in logical_sets:\n",
    "    if not logical_set in unique_list:\n",
    "        unique_list.append(logical_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through these categories and download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current category Done - Level 1\n",
      "current category Incomplete Evaluation\n",
      "current category Incomplete Evaluation - Unclaimed\n",
      "current category Done - Level 2 Required\n",
      "current category Exempt - engineer's letter\n",
      "current category Exempt - no large openings\n",
      "current category Level 1 - missing data\n",
      "current category Level 2 - Missing slope\n",
      "current category Exempt - less than 5 units\n",
      "current category Exempt - retrofitted\n",
      "current category Ask for exemption - did not review\n",
      "current category Exempt - city inspection\n",
      "current category Done - Level 2 in process\n",
      "current category Exempt - no parking/commercial\n",
      "current category Incomplete Evaluation\n",
      "current category Done - Level 2 in process - requires retrofit\n",
      "current category Exempt - Garage In basement\n",
      "current category Exempt - built after 1990\n"
     ]
    }
   ],
   "source": [
    "for item in unique_list:\n",
    "    print('current category', item[0])\n",
    "    download_images(df_vul[df_vul['status_long'].isin(item)],\n",
    "                           class_name='vulnerable',\n",
    "                           category = item[0],test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All done - we got our training dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
